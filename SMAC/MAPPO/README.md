# MAPPO in StarCraft II Environment
This is a concise Pytorch implementation of MAPPO in StarCraft II environment(SMAC-StarCraft Multi-Agent Challenge).<br />


## How to use?
Run 'MAPPO_SMAC_main.py' in your own IDE.<br />

## Trainning environments
Modify the env_names list in the codes to change the maps in StarCraft II. 

## Requirements
python>=3.7.9<br />
numpy>=1.21.6<br />
pytorch>=1.12.1<br />
tensorboard>=2.11.2<br />
[SMAC-StarCraft Multi-Agent Challenge](https://github.com/oxwhirl/smac)

## Reference
[1] Yu C, Velu A, Vinitsky E, et al. The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games[J]. arXiv preprint arXiv:2103.01955, 2021.<br />
[2] [Official implementation of MAPPO](https://github.com/marlbenchmark/on-policy) <br />
[3] [EPyMARL](https://github.com/uoe-agents/epymarl)
